{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53791219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='./datasets/path_to_data'\n",
    "annFile='./datasets/path_to_data'\n",
    "\n",
    "# Initialize the COCO api for instance annotations\n",
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = coco.getCatIds()\n",
    "num_categories = len(category_ids)\n",
    "print('number of categories: ',num_categories)\n",
    "for ids in category_ids:\n",
    "    cats = coco.loadCats(ids=ids)\n",
    "    print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f111d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = coco.getImgIds()\n",
    "print(len(image_ids))\n",
    "image_id = image_ids[5]\n",
    "image_info = coco.loadImgs(image_id)\n",
    "print(image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations for the given ids\n",
    "annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "#print(len(annotation_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterClasses = ['IC', 'Capacitor', 'Resistor']\n",
    "catIds = coco.getCatIds(catNms=filterClasses)\n",
    "print(catIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "catID = 2\n",
    "print(coco.loadCats(ids=catID))\n",
    "\n",
    "imgId = coco.getImgIds(catIds=[catID])[4]\n",
    "print(imgId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ids = coco.getAnnIds(imgIds=[imgId], iscrowd=None)\n",
    "print(ann_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40896d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Annotations for Image ID {imgId}:\")\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "image_path = dataDir + coco.loadImgs(imgId)[0]['file_name']\n",
    "print(image_path)\n",
    "image = plt.imread(image_path)\n",
    "plt.imshow(image)\n",
    "\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Annotations for Image ID: {}'.format(image_id))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    cat_ids = coco.getCatIds()\n",
    "    print(f\"Number of Unique Categories: {len(cat_ids)}\")\n",
    "    print(\"Category IDs:\")\n",
    "    print(cat_ids)  # The IDs are not necessarily consecutive.\n",
    "\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    cat_names = [cat[\"name\"] for cat in cats]\n",
    "    print(\"Categories Names:\")\n",
    "    print(cat_names)\n",
    "\n",
    "    query_id = cat_ids[0]\n",
    "    query_annotation = coco.loadCats([query_id])[0]\n",
    "    query_name = query_annotation[\"name\"]\n",
    "    print(\"Category ID -> Category Name:\")\n",
    "    print(f\"Category ID: {query_id}, Category Name: {query_name}\")\n",
    "\n",
    "    query_name = cat_names[2]\n",
    "    query_id = coco.getCatIds(catNms=[query_name])[0]\n",
    "    print(\"Category Name -> ID:\")\n",
    "    print(f\"Category Name: {query_name}, Category ID: {query_id}\")\n",
    "\n",
    "    img_ids = coco.getImgIds(catIds=[query_id])\n",
    "    print(f\"Number of Images Containing {query_name}: {len(img_ids)}\")\n",
    "\n",
    "    img_id = img_ids[2]\n",
    "    img_info = coco.loadImgs([img_id])[0]\n",
    "    img_file_name = img_info[\"file_name\"]\n",
    "    print(f\"Image ID: {img_id}, File Name: {img_file_name}\")\n",
    "\n",
    "    ann_ids = coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    print(f\"Annotations for Image ID {img_id}:\")\n",
    "    print(anns)\n",
    "\n",
    "    im = plt.imread(dataDir + coco.loadImgs(img_id)[0]['file_name'])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.asarray(im))\n",
    "    plt.savefig(f\"{img_id}.jpg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    plt.savefig(f\"{img_id}_annotated.jpg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catIDs = coco.getCatIds()\n",
    "cats = coco.loadCats(catIDs)\n",
    "\n",
    "category_names = [cat['name'].title() for cat in cats]\n",
    "\n",
    "category_counts = [coco.getImgIds(catIds=[cat['id']]) for cat in cats]\n",
    "category_counts = [len(img_ids) for img_ids in category_counts]\n",
    "\n",
    "\n",
    "colors = sns.color_palette('viridis', len(category_names))\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(x=category_counts, y=category_names, palette=colors)\n",
    "\n",
    "for i, count in enumerate(category_counts):\n",
    "    plt.text(count + 20, i, str(count), va='center')\n",
    "plt.xlabel('Count',fontsize=20)\n",
    "plt.ylabel('Category',fontsize=20)\n",
    "plt.title('Category Distribution in PCB Dataset',fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig('coco-cats.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(category_counts)\n",
    "category_percentages = [(count / total_count) * 100 for count in category_counts]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "\n",
    "labels = [f\"{name} \" for name, percentage in zip(category_names, category_percentages)]\n",
    "label_props = {\"fontsize\": 25, \n",
    "               \"bbox\": {\"edgecolor\": \"white\", \n",
    "                        \"facecolor\": \"white\", \n",
    "                        \"alpha\": 0.7, \n",
    "                        \"pad\": 0.5}\n",
    "              }\n",
    "\n",
    "wedges, _, autotexts = plt.pie(category_counts, \n",
    "                              autopct='', \n",
    "                              startangle=90, \n",
    "                              textprops=label_props, \n",
    "                              pctdistance=0.85)\n",
    "\n",
    "legend_labels = [f\"{label}\\n{category_percentages[i]:.1f}%\" for i, label in enumerate(labels)]\n",
    "plt.legend(wedges, legend_labels, title=\"Categories\", loc=\"upper center\", bbox_to_anchor=(0.5, -0.01), \n",
    "           ncol=4, fontsize=12)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Category Distribution in COCO Dataset', fontsize=29)\n",
    "plt.tight_layout()\n",
    "plt.savefig('coco-dis.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count # the above analysis is wrong. somewhere data was misread or labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad05745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import funcy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python cocosplit.py -s 0.7 result.json train.json test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "ANNOTATION_FILE_TRAIN = 'data/train.json'\n",
    "ANNOTATION_FILE_VAL = 'data/test.json'\n",
    "\n",
    "coco_train = COCO(ANNOTATION_FILE_TRAIN)\n",
    "catIds_train = coco_train.getCatIds()\n",
    "imgIds_train = coco_train.getImgIds()\n",
    "imgDict_train = coco_train.loadImgs(imgIds_train)\n",
    "\n",
    "coco_val = COCO(ANNOTATION_FILE_VAL)\n",
    "catIds_val = coco_val.getCatIds()\n",
    "imgIds_val = coco_val.getImgIds()\n",
    "imgDict_val = coco_val.loadImgs(imgIds_val)\n",
    "\n",
    "print(len(imgIds_train), len(catIds_train))\n",
    "print(len(imgIds_val), len(catIds_val))\n",
    "print(catIds_val)\n",
    "print(coco_val.getImgIds())\n",
    "print(coco_train.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59acf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d07864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tqdm import tqdm\n",
    "\n",
    "### using ADAM model --> terrible results / needs debugging\n",
    "\n",
    "def load_coco_dataset(coco_annotations_file, image_dir):\n",
    "    with open(coco_annotations_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    images = coco_data['images']\n",
    "    annotations = coco_data['annotations']\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for image in tqdm(images, desc=\"Loading images\"):\n",
    "        image_id = image['id']\n",
    "        img_path = os.path.join(image_dir, image['file_name'])\n",
    "        category_id = [annotation['category_id'] for annotation in annotations if annotation['image_id'] == image_id]\n",
    "        if category_id:  # Ensure image has at least one annotation\n",
    "            X.append(img_path)\n",
    "            y.append(category_id[0])  # Take the first category as the label\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def preprocess_images(X):\n",
    "    X_processed = []\n",
    "    for img_path in tqdm(X, desc=\"Preprocessing images\"):\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array /= 255.0  # Normalize pixel values\n",
    "        X_processed.append(img_array)\n",
    "    return np.array(X_processed)\n",
    "\n",
    "COCO_ANNOTATIONS_FILE = 'result.json'\n",
    "IMAGE_DIR = './datasets/path_to_images'\n",
    "X, y = load_coco_dataset(COCO_ANNOTATIONS_FILE, IMAGE_DIR)\n",
    "\n",
    "X = preprocess_images(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(80, activation='softmax')  # Assuming 80 COCO classes\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # (number samples, height, width, channels (RBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e11bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5063106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape # only holds labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4347f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8541c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_mobilenet_v3_large_fpn,\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "\n",
    "def get_faster_rcnn_model(num_classes):\n",
    "    \"\"\"return model and preprocessing transform\"\"\"\n",
    "    model = fasterrcnn_mobilenet_v3_large_fpn(\n",
    "        weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "    )\n",
    "    model.roi_heads.box_predictor.cls_score = torch.nn.Linear(\n",
    "        in_features=model.roi_heads.box_predictor.cls_score.in_features,\n",
    "        out_features=num_classes,\n",
    "        bias=True,\n",
    "    )\n",
    "    model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(\n",
    "        in_features=model.roi_heads.box_predictor.bbox_pred.in_features,\n",
    "        out_features=num_classes * 4,\n",
    "        bias=True,\n",
    "    )\n",
    "    preprocess = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT.transforms()\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "model, preprocess = get_faster_rcnn_model(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470925c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for COCO annotations.\"\"\"\n",
    "\n",
    "    # adapted from https://github.com/pytorch/vision/issues/2720\n",
    "\n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        \"\"\"Load COCO annotation data.\"\"\"\n",
    "        self.data_dir = Path(root)\n",
    "        self.transform = transform\n",
    "\n",
    "        # load the COCO annotations json\n",
    "        anno_file_path = annFile\n",
    "        with open(str(anno_file_path)) as file_obj:\n",
    "            self.coco_data = json.load(file_obj)\n",
    "        # put all of the annos into a dict where keys are image IDs to speed up retrieval\n",
    "        self.image_id_to_annos = defaultdict(list)\n",
    "        for anno in self.coco_data[\"annotations\"]:\n",
    "            image_id = anno[\"image_id\"]\n",
    "            self.image_id_to_annos[image_id] += [anno]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_data[\"images\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return tuple of image and labels as torch tensors.\"\"\"\n",
    "        image_data = self.coco_data[\"images\"][index]\n",
    "        image_id = image_data[\"id\"]\n",
    "        image_path = self.data_dir / image_data[\"file_name\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        annos = self.image_id_to_annos[image_id]\n",
    "        anno_data = {\n",
    "            \"boxes\": [],\n",
    "            \"labels\": [],\n",
    "            \"area\": [],\n",
    "            \"iscrowd\": [],\n",
    "        }\n",
    "        for anno in annos:\n",
    "            coco_bbox = anno[\"bbox\"]\n",
    "            left = coco_bbox[0]\n",
    "            top = coco_bbox[1]\n",
    "            right = coco_bbox[0] + coco_bbox[2]\n",
    "            bottom = coco_bbox[1] + coco_bbox[3]\n",
    "            area = coco_bbox[2] * coco_bbox[3]\n",
    "            anno_data[\"boxes\"].append([left, top, right, bottom])\n",
    "            anno_data[\"labels\"].append(anno[\"category_id\"])\n",
    "            anno_data[\"area\"].append(area)\n",
    "            anno_data[\"iscrowd\"].append(anno[\"iscrowd\"])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(anno_data[\"boxes\"], dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(anno_data[\"labels\"], dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"area\": torch.as_tensor(anno_data[\"area\"], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.as_tensor(anno_data[\"iscrowd\"], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import display\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# create datasets\n",
    "training_dataset = CocoDataset(\n",
    "    root=\"./\",\n",
    "    annFile=\"test.json\",\n",
    "    transform=preprocess,\n",
    ")\n",
    "validation_dataset = CocoDataset(\n",
    "    root=\"./\",\n",
    "    annFile=\"train.json\",\n",
    "    transform=preprocess,\n",
    ")\n",
    "\n",
    "print(f\"training dataset size: {training_dataset.__len__()}\")\n",
    "\n",
    "print(f\"validation dataset size: {validation_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = training_dataset[random.randint(0, len(training_dataset) - 1)]\n",
    "print(f\"random training label: {label}\")\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "img = transform(img)\n",
    "x1, y1, x2, y2 = label[\"boxes\"].numpy()[0]\n",
    "draw = ImageDraw.Draw(img)\n",
    "draw.rectangle([x1, y1, x2, y2], fill=None, outline=\"#ff0000cc\", width=2)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\"return tuple data\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437815e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1740742",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    params, \n",
    "    lr=0.001, \n",
    "    momentum=0.9, \n",
    "    weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for images, targets in train_loader:\n",
    "#    print(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    N = len(train_loader.dataset)\n",
    "    current_train_loss = 0\n",
    "    # train loop\n",
    "    for images, targets in train_loader:\n",
    "#        images = list(image.to(device) for image in images)\n",
    "#        targets = [\n",
    "#            {\n",
    "#                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "#                for k, v in t.items()\n",
    "#            }\n",
    "#            for t in targets\n",
    "#        ]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += losses\n",
    "    train_loss_list.append(current_train_loss / N)\n",
    "\n",
    "    # validation loop\n",
    "    N = len(validation_loader.dataset)\n",
    "    current_validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "#        for images, targets in validation_loader:\n",
    "#            images = list(image.to(device) for image in images)\n",
    "#            targets = [\n",
    "#                {\n",
    "#                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "#                    for k, v in t.items()\n",
    "#                }\n",
    "#                for t in targets\n",
    "#            ]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            current_validation_loss += losses\n",
    "    validation_loss_list.append(current_validation_loss / N)\n",
    "\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(\n",
    "        f\"train loss: {train_loss_list[-1]}, validation loss: {validation_loss_list[-1]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67345fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./model.pth\") # save model to file\n",
    "\n",
    "# plot losses\n",
    "train_loss = [x.cpu().detach().numpy() for x in train_loss_list]\n",
    "validation_loss = [x.cpu().detach().numpy() for x in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss, \"-o\", label=\"train loss\")\n",
    "plt.plot(validation_loss, \"-o\", label=\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model.pth\")\n",
    "\n",
    "def inference(img, model):\n",
    "    model.eval()\n",
    "    pred = model([img]) # forward pass\n",
    "\n",
    "    transform = T.ToPILImage()\n",
    "    img = transform(img)\n",
    "    x1, y1, x2, y2 = pred[0][\"boxes\"].cpu().detach().numpy()[0]\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle([x1, y1, x2, y2], fill=None, outline=\"#ff0000cc\", width=2)\n",
    "    display(img)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = validation_dataset[random.randint(0, len(validation_dataset) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(img, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce18f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = ['Capacitor', 'IC', 'Resistor']\n",
    "#labels = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = ['Capacitors', 'Resistors', 'Integrated Circuits']\n",
    "values = [149, 133, 24]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(values, labels=categories, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Component Annotations for PCB Dataset')\n",
    "plt.axis('equal') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
